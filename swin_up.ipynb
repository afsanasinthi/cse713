{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoANfrAbjdX8DUZv5Uqm7M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A2ppe9y-SFA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOY3_1oh_PCJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4ybvU003K-g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T # for simplifying the transforms\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShSFYGUo3Qd-"
      },
      "outputs": [],
      "source": [
        "!pip install timm # kaggle doesnt have it installed by default\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBLaN-wa3Ubd"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuNcMd5n3f2G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BjisnnI3h_c"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjc4X-Xi3k0O"
      },
      "outputs": [],
      "source": [
        "def get_classes(data_dir):\n",
        "    all_data = datasets.ImageFolder(data_dir)\n",
        "    return all_data.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVCXqtr_33hz"
      },
      "outputs": [],
      "source": [
        "def get_data_loaders(data_dir, batch_size, train = False):\n",
        "    if train:\n",
        "        #train\n",
        "        transform = T.Compose([\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomVerticalFlip(),\n",
        "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n",
        "            T.Resize(256),\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD), # imagenet means\n",
        "            T.RandomErasing(p=0.1, value='random')\n",
        "        ])\n",
        "        train_data = datasets.ImageFolder(os.path.join(data_dir, \"train/\"), transform = transform)\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        return train_loader, len(train_data)\n",
        "    else:\n",
        "        # val/test\n",
        "        transform = T.Compose([ # We dont need augmentation for test transforms\n",
        "            T.Resize(256),\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(timm.data.IMAGENET_DEFAULT_MEAN, timm.data.IMAGENET_DEFAULT_STD), # imagenet means\n",
        "        ])\n",
        "        val_data = datasets.ImageFolder(os.path.join(data_dir, \"valid/\"), transform=transform)\n",
        "        test_data = datasets.ImageFolder(os.path.join(data_dir, \"test/\"), transform=transform)\n",
        "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        return val_loader, test_loader, len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGcnNhuI4AC8"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/sunflowerdataset/Augmented Image/new_dataset_v1/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSl7HjUv5RY5"
      },
      "outputs": [],
      "source": [
        "(train_loader, train_data_len) = get_data_loaders(dataset_path, 128, train=True)\n",
        "(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loaders(dataset_path, 32, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45tcAhDu5UQ1"
      },
      "outputs": [],
      "source": [
        "classes = get_classes(\"/content/drive/MyDrive/sunflowerdataset/Augmented Image/new_dataset_v1/train/\")\n",
        "print(classes, len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS82v1p95Z-G"
      },
      "outputs": [],
      "source": [
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": val_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    \"train\": train_data_len,\n",
        "    \"val\": valid_data_len\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIe63rOa5yx3"
      },
      "outputs": [],
      "source": [
        "print(len(train_loader), len(val_loader), len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47W5o6sC52my"
      },
      "outputs": [],
      "source": [
        "print(train_data_len, valid_data_len, test_data_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70CCpRJ16fYx"
      },
      "outputs": [],
      "source": [
        "# now, for the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z0mDrwA6kCi"
      },
      "outputs": [],
      "source": [
        "HUB_URL = \"SharanSMenon/swin-transformer-hub:main\"\n",
        "MODEL_NAME = \"swin_tiny_patch4_window7_224\"\n",
        "# check hubconf for more models.\n",
        "model = torch.hub.load(HUB_URL, MODEL_NAME, pretrained=True) # load from torch hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou9jWaH26nX2"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters(): #freeze model\n",
        "    param.requires_grad = False\n",
        "\n",
        "n_inputs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, len(classes))\n",
        ")\n",
        "model = model.to(device)\n",
        "print(model.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwClKm7f6ptG"
      },
      "outputs": [],
      "source": [
        "criterion = LabelSmoothingCrossEntropy()\n",
        "criterion = criterion.to(device)\n",
        "optimizer = optim.AdamW(model.head.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeLuDXCA6yjY"
      },
      "outputs": [],
      "source": [
        "# lr scheduler\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxTDf7gK60nn"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print(\"-\"*10)\n",
        "        \n",
        "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
        "            if phase == 'train':\n",
        "                model.train() # model to training mode\n",
        "            else:\n",
        "                model.eval() # model to evaluate\n",
        "            \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "            \n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                scheduler.step() # step at end of epoch\n",
        "            \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
        "        print()\n",
        "    time_elapsed = time.time() - since # slight error\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
        "    \n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUjeNdvJ64Sv"
      },
      "outputs": [],
      "source": [
        "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=50) # now it is a lot faster\n",
        "# I will come back after 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = preds(test_set, test_set.samples / batch_size)\n",
        "val_preds = np.argmax(Y_pred, axis=1)\n",
        "import sklearn.metrics as metrics\n",
        "val_trues =test_set.classes\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(val_trues, val_preds))"
      ],
      "metadata": {
        "id": "kGg980DYjxl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2Jio4dI67ce"
      },
      "outputs": [],
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0 for i in range(len(classes)))\n",
        "class_total = list(0 for i in range(len(classes)))\n",
        "model_ft.eval()\n",
        "\n",
        "for data, target in tqdm(test_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    with torch.no_grad(): # turn off autograd for faster testing\n",
        "        output = model_ft(data)\n",
        "        loss = criterion(output, target)\n",
        "    test_loss = loss.item() * data.size(0)\n",
        "    _, pred = torch.max(output, 1)\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    if len(target) == 32:\n",
        "        for i in range(32):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "test_loss = test_loss / test_data_len\n",
        "print('Test Loss: {:.4f}'.format(test_loss))\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
        "            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
        "        ))\n",
        "    else:\n",
        "        print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
        "print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
        "            100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCaG097t7I2_"
      },
      "outputs": [],
      "source": [
        "# our model earns 93% test accuracy, which is very high. lets save it\n",
        "example = torch.rand(1, 3, 224, 224)\n",
        "traced_script_module = torch.jit.trace(model.cpu(), example)\n",
        "traced_script_module.save(\"swin_transformer.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5osnvA1aY0x"
      },
      "outputs": [],
      "source": []
    }
  ]
}