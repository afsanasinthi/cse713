{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAkP2eJE6atUTkAj+UqXEg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoObRYFNV6nb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgCkFDThIqgB"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T # for simplifying the transforms\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "from torchvision import models"
      ],
      "metadata": {
        "id": "QzWx9ahng0mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now, we import timm, torchvision image models\n",
        "!pip install timm # kaggle doesnt have it installed by default\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss"
      ],
      "metadata": {
        "id": "OR1QuaHng3X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "9D5x39meg6HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "TcGJfHPzg8XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy"
      ],
      "metadata": {
        "id": "wxdqM6mBg9s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classes(data_dir):\n",
        "    all_data = datasets.ImageFolder(data_dir)\n",
        "    return all_data.classes"
      ],
      "metadata": {
        "id": "jZVCE5NUg_pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loaders(data_dir, batch_size, train = False):\n",
        "    if train:\n",
        "        #train\n",
        "        transform = T.Compose([\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomVerticalFlip(),\n",
        "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter()]), p=0.25),\n",
        "            T.Resize(256),\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n",
        "            T.RandomErasing(p=0.2, value='random')\n",
        "        ])\n",
        "        train_data = datasets.ImageFolder(os.path.join(data_dir, \"train/\"), transform = transform)\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        return train_loader, len(train_data)\n",
        "    else:\n",
        "        # val/test\n",
        "        transform = T.Compose([ # We dont need augmentation for test transforms\n",
        "            T.Resize(256),\n",
        "            T.CenterCrop(224),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # imagenet means\n",
        "        ])\n",
        "        val_data = datasets.ImageFolder(os.path.join(data_dir, \"valid/\"), transform=transform)\n",
        "        test_data = datasets.ImageFolder(os.path.join(data_dir, \"test/\"), transform=transform)\n",
        "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        return val_loader, test_loader, len(val_data), len(test_data)\n"
      ],
      "metadata": {
        "id": "wIdO_ydGhFcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/sunflowerdataset/Augmented Image/new_dataset_v1/\""
      ],
      "metadata": {
        "id": "ZvVN5SJjhIE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_loader, train_data_len) = get_data_loaders(dataset_path, 128, train=True)\n",
        "(val_loader, test_loader, valid_data_len, test_data_len) = get_data_loaders(dataset_path, 32, train=False)"
      ],
      "metadata": {
        "id": "7qCEYtJuhSMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_loader)"
      ],
      "metadata": {
        "id": "DtldLJTWoPBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = get_classes(\"/content/drive/MyDrive/sunflowerdataset/Augmented Image/new_dataset_v1/train\")\n",
        "print(classes, len(classes))"
      ],
      "metadata": {
        "id": "je6Rz1c0hUIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": val_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    \"train\": train_data_len,\n",
        "    \"val\": valid_data_len\n",
        "}"
      ],
      "metadata": {
        "id": "NSjaZTcPktn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_loader), len(val_loader), len(test_loader))"
      ],
      "metadata": {
        "id": "w3mI4LtEkzO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data_len, valid_data_len, test_data_len)"
      ],
      "metadata": {
        "id": "yZB_xCb6k1J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, for the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "fYwJs9aqk27u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)"
      ],
      "metadata": {
        "id": "7PLEjrVnk5rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters(): #freeze model\n",
        "    param.requires_grad = False\n",
        "\n",
        "n_inputs = model.head.in_features\n",
        "model.head = nn.Sequential(\n",
        "    nn.Linear(n_inputs, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, len(classes))\n",
        ")\n",
        "model = model.to(device)\n",
        "print(model.head)"
      ],
      "metadata": {
        "id": "fcyRMD6ylAhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = LabelSmoothingCrossEntropy()\n",
        "criterion = criterion.to(device)\n",
        "optimizer = optim.Adam(model.head.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "egbhMxP0lDiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr scheduler\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.97)"
      ],
      "metadata": {
        "id": "i4cbi3LUlIOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print(\"-\"*10)\n",
        "        \n",
        "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
        "            if phase == 'train':\n",
        "                model.train() # model to training mode\n",
        "            else:\n",
        "                model.eval() # model to evaluate\n",
        "            \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "            \n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                scheduler.step() # step at end of epoch\n",
        "            \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
        "        print()\n",
        "    time_elapsed = time.time() - since # slight error\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
        "    \n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "drU1k9gPlKLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler) # now it is a lot faster\n",
        "# I will come back after 50 epochs"
      ],
      "metadata": {
        "id": "NpW2DvyFlOe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0 for i in range(len(classes)))\n",
        "class_total = list(0 for i in range(len(classes)))\n",
        "model.eval()\n",
        "\n",
        "for data, target in tqdm(test_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    with torch.no_grad(): # turn off autograd for faster testing\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "    test_loss = loss.item() * data.size(0)\n",
        "    _, pred = torch.max(output, 1)\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    if len(target) == 32:\n",
        "        for i in range(32):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "test_loss = test_loss / test_data_len\n",
        "print('Test Loss: {:.4f}'.format(test_loss))\n",
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
        "            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
        "        ))\n",
        "    else:\n",
        "        print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
        "print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
        "            100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
        "        ))"
      ],
      "metadata": {
        "id": "M2GgWLUPrqUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our model earns 93% test accuracy, which is very high. lets save it\n",
        "example = torch.rand(1, 3, 224, 224)\n",
        "traced_script_module = torch.jit.trace(model.cpu(), example)\n",
        "traced_script_module.save(\"sunflower.pt\")"
      ],
      "metadata": {
        "id": "5MJDUadNsaUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications.imagenet_utils import decode_predictions"
      ],
      "metadata": {
        "id": "3x75lM-o2WOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep scikit-image\n",
        "     "
      ],
      "metadata": {
        "id": "Fa-bouXv2Y6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "\n",
        "EFFICIENTNET_VERSION = {\n",
        "    'B0': {'model': tf.keras.applications.EfficientNetB0, 'img_size': 224},\n",
        "    'B1': {'model': tf.keras.applications.EfficientNetB1, 'img_size': 240},\n",
        "    'B2': {'model': tf.keras.applications.EfficientNetB2, 'img_size': 260},\n",
        "    'B3': {'model': tf.keras.applications.EfficientNetB3, 'img_size': 300},\n",
        "    'B4': {'model': tf.keras.applications.EfficientNetB4, 'img_size': 380},\n",
        "    'B5': {'model': tf.keras.applications.EfficientNetB5, 'img_size': 456},\n",
        "    'B6': {'model': tf.keras.applications.EfficientNetB6, 'img_size': 528},\n",
        "    'B7': {'model': tf.keras.applications.EfficientNetB7, 'img_size': 600},\n",
        "}\n",
        "\n",
        "\n",
        "def get_efficientnet(\n",
        "        version: str = 'B0', weights: str = 'imagenet'\n",
        ") -> Tuple[tf.keras.Model, callable]:\n",
        "    \"\"\"\n",
        "    Function to get one of the EfficientNet models with pretrained weights. For\n",
        "    more information on image resolution and number of parameters for each of\n",
        "    the EfficientNet models see:\n",
        "    https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n",
        "    \"\"\"\n",
        "    img_size = EFFICIENTNET_VERSION[version]['img_size']\n",
        "    print(img_size)\n",
        "    def preprocess_image(x):\n",
        "        return tf.image.resize(x, (img_size, img_size))\n",
        "\n",
        "    return (\n",
        "        EFFICIENTNET_VERSION[version]['model'](weights=weights),\n",
        "        preprocess_image\n",
        "    )"
      ],
      "metadata": {
        "id": "aiHJXF6b2cn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('..')"
      ],
      "metadata": {
        "id": "xvUn858C2m5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get model and corresponding preprocess function\n",
        "model, preprocess_image = get_efficientnet(version='B0', weights='imagenet')"
      ],
      "metadata": {
        "id": "bsF0qJIM2pGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "d8PQ1eFW2rYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = PIL.Image.open('/content/drive/MyDrive/sunflowerdataset/Augmented Image/new_dataset_v1/test/Downy_mildew/DownyMildew (470).png') \n",
        "img = preprocess_image(np.array(img).astype(int))\n",
        "plt.imshow(img.numpy().astype(int));"
      ],
      "metadata": {
        "id": "JvOavGcA2tCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions (expand dims for single image as model expects size (None, 224, 224, 3))\n",
        "preds = model(np.expand_dims(img, 0))"
      ],
      "metadata": {
        "id": "WiuTXMyl24ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decode it based on keras imagenet_utils package\n",
        "decoded_preds = decode_predictions(preds.numpy())[0]\n",
        "print(f\"Predicted species: {decoded_preds[0][1]} | Probability: {decoded_preds[0][2]:.2f}\")"
      ],
      "metadata": {
        "id": "gBXiIWFR27vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "sys.path.insert(0, '/content')\n",
        "%cd /content/\n",
        "%pwd"
      ],
      "metadata": {
        "id": "GY24-Vef29kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = preds(test_set, test_set.samples / batch_size)\n",
        "val_preds = np.argmax(Y_pred, axis=1)\n",
        "import sklearn.metrics as metrics\n",
        "val_trues =test_set.classes\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(val_trues, val_preds))"
      ],
      "metadata": {
        "id": "aT-zHd_73DPP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}